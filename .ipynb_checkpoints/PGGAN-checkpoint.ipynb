{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712f5d1f-3917-419a-a643-cc8fafb16cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a583d4-c21c-4606-b835-caa4263720b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bf1aae-e216-4220-9f51-cf02fde8a2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = '/media/kati/drive1TB/bg-20k/train_images'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a43b70d-a316-4c67-9c18-69f7067325bb",
   "metadata": {},
   "source": [
    "# Load Data & Augmenting images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa09a125-83ba-4493-86b2-93e7e6282391",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cutout(object):\n",
    "    def __init__(self, box_length):\n",
    "        self.box_length = box_length\n",
    "\n",
    "    def __call__(self, image):\n",
    "        # Get the dimensions of the image\n",
    "        _, height, width = image.size()\n",
    "    \n",
    "        # Generate random coordinates for the top-left corner of the rectangle\n",
    "        x1 = np.random.randint(0, width - self.box_length)\n",
    "        y1 = np.random.randint(0, height - self.box_length)\n",
    "    \n",
    "        # Calculate the coordinates of the bottom-right corner of the rectangle\n",
    "        x2 = x1 + self.box_length\n",
    "        y2 = y1 + self.box_length\n",
    "    \n",
    "        # Apply the mask to the specified region\n",
    "        image[:, y1:y2, x1:x2] = 255\n",
    "    \n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c307f9-eba6-4e14-9192-68ce9b7889a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedDataset(Dataset):\n",
    "    def __init__(self, dataset_arg, dataset_ori):\n",
    "        assert len(dataset_arg) == len(dataset_ori), \"Datasets must have the same length\"\n",
    "        self.dataset_arg = dataset_arg\n",
    "        self.dataset_ori = dataset_ori\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.dataset_arg[index][0]  # Get image from dataset_arg\n",
    "        y = self.dataset_ori[index][0]  # Get image from dataset_ori\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83877d96-d1b2-41a0-8945-b2c18720fb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 100\n",
    "HEIGHT = 100 \n",
    "transform_arg = transforms.Compose([\n",
    "    transforms.PILToTensor(),\n",
    "    transforms.Resize(size=(WIDTH, HEIGHT), antialias=True),\n",
    "    Cutout(40),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize(mean=[0.4338, 0.4341, 0.4100], std=[0.3002, 0.2798, 0.3021]), #  Normalize with BG-20k stats\n",
    "])\n",
    "\n",
    "transform_ori = transforms.Compose([\n",
    "    transforms.PILToTensor(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Resize(size=(WIDTH, HEIGHT), antialias=True),\n",
    "    transforms.Normalize(mean=[0.4338, 0.4341, 0.4100], std=[0.3002, 0.2798, 0.3021]), #  Normalize with BG-20k stats\n",
    "])\n",
    "\n",
    "def denormalize(tensor, mean=[0.4338, 0.4341, 0.4100], std=[0.3002, 0.2798, 0.3021]):\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return tensor.clamp(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fb1f61-7a71-4eb2-9fc8-d82a2bbd6ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "dataset_arg = datasets.ImageFolder(IMAGE_PATH, transform=transform_arg)\n",
    "dataset_ori = datasets.ImageFolder(IMAGE_PATH, transform=transform_ori)\n",
    "\n",
    "paired_dataset = PairedDataset(dataset_arg, dataset_ori)\n",
    "\n",
    "dataloader = DataLoader(paired_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4946d326-5c9b-42e1-b9a2-f5d0d87e4d21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x, y in dataloader:\n",
    "    print(x.shape)\n",
    "    x = denormalize(x[0])\n",
    "    y = denormalize(y[0])\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    axs[0].imshow(x.permute(1, 2, 0))\n",
    "    axs[1].imshow(y.permute(1, 2, 0))\n",
    "    axs[1].axis(False)\n",
    "    axs[0].axis(False)\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b09f8b8-18ce-4dae-8e0f-b35fdfb7dc39",
   "metadata": {},
   "source": [
    "# Patch-Based Image Inpainting with Generative Adversarial Networks (Demir and Unal, 2018): \n",
    "based of this [paper](https://arxiv.org/pdf/1803.07422.pdf) by Ugur Demir, Gozde Unal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87bb2e6-16eb-49b7-835a-0b2cd5af9557",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa220ad-778e-4b5b-817c-613c0617aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding='same'),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv(x)\n",
    "\n",
    "class RasNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, hidden_channels=32):\n",
    "        super().__init__()\n",
    "        self.downsample = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_channels, kernel_size=9, padding='same'),\n",
    "            nn.BatchNorm2d(hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_channels, hidden_channels*2, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(hidden_channels*2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.residual_block = nn.Sequential(\n",
    "            ResidualBlock(hidden_channels*2),\n",
    "            ResidualBlock(hidden_channels*2),\n",
    "            ResidualBlock(hidden_channels*2),\n",
    "            ResidualBlock(hidden_channels*2),\n",
    "            ResidualBlock(hidden_channels*2),\n",
    "            ResidualBlock(hidden_channels*2),\n",
    "        )\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.ConvTranspose2d(hidden_channels*2, hidden_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(hidden_channels, in_channels, kernel_size=9, padding=4),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.downsample(x)\n",
    "        x = self.residual_block(x)\n",
    "        x = self.upsample(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beba1b42-dfdf-4772-9719-9eb4ea7ade24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedLayer(nn.Module):\n",
    "    def __init__(self, in_channels=3, hidden_channels=32):\n",
    "        super().__init__()\n",
    "        self.shared_layer = nn.Sequential(\n",
    "            self.make_shared_block(in_channels, hidden_channels),\n",
    "            self.make_shared_block(hidden_channels, hidden_channels*2),\n",
    "            self.make_shared_block(hidden_channels*2, hidden_channels*4),\n",
    "            self.make_shared_block(hidden_channels*4, hidden_channels*8),\n",
    "        )\n",
    "        \n",
    "    @staticmethod\n",
    "    def make_shared_block(in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.shared_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911b7040-e4db-49c0-9339-7e110e57783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalDiscriminator(nn.Module):\n",
    "    def __init__(self, hidden_channels=32):\n",
    "        super().__init__()\n",
    "        self.global_path = nn.Sequential(\n",
    "            nn.Conv2d(hidden_channels * 8, hidden_channels * 16, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(hidden_channels * 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(hidden_channels * 16, 1, kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.global_path(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38062552-7faf-486f-b822-9d017b26d4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, hidden_channels=32):\n",
    "        super().__init__()\n",
    "        self.patch_gan = nn.Sequential(\n",
    "            nn.Conv2d(hidden_channels * 8, hidden_channels * 16, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(hidden_channels * 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(hidden_channels * 16, 1, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_gan(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c9cc9a-0442-4663-92e2-b7ec9c540353",
   "metadata": {},
   "source": [
    "# Compute Loss\n",
    "simplified $$L_{rec} = L_{1}Loss$$\n",
    "$$L_{adv} = BCELoss$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c58355d-f19f-4036-af81-914e87e55e3a",
   "metadata": {},
   "source": [
    "## Generator Loss\n",
    "We update the generator parameters by joint loss $$L_{joint} = \\lambda_1L_{rec} + \\lambda_2L_{g-adv} + \\lambda_3L_{p-adv}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96712a8b-d916-4e67-8644-72da089d8905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gen_loss(input_image, real_image, gen, shared, global_disc, patch_disc, recon_criterion, adver_criterion, lambdas=[0.995, 0.0025, 0.0025]):\n",
    "    fake_image = gen(input_image) # Pass RasNet\n",
    "    l_rec = recon_criterion(fake_image, real_image)\n",
    "\n",
    "    # To PGGAN Discriminator\n",
    "    shared_fake_output = shared(fake_image)\n",
    "\n",
    "    # Global path\n",
    "    global_fake_pred = global_disc(shared_fake_output)\n",
    "    l_g_adv = adver_criterion(global_fake_pred, torch.ones_like(global_fake_pred))\n",
    "\n",
    "    # Patch path\n",
    "    patch_fake_pred = patch_disc(shared_fake_output)\n",
    "    l_p_adv = adver_criterion(patch_fake_pred, torch.ones_like(patch_fake_pred))\n",
    "    \n",
    "    return lambdas[0]*l_rec + lambdas[1]*l_g_adv + lambdas[2]*l_p_adv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef54c8d8-98a5-436f-8adb-48b58c52329e",
   "metadata": {},
   "source": [
    "## Global Driscriminator Loss\n",
    "by $$L_{g-adv} = BCELoss$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebddca2-e586-42d6-b008-5f4c29f5ce2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_disc_loss(input_image, real_image, gen, shared, global_disc, adver_criterion):\n",
    "    fake_image = gen(input_image)\n",
    "\n",
    "    shared_fake_output = shared(fake_image.detach())\n",
    "    shared_real_output = shared(real_image)\n",
    "    \n",
    "    global_fake_pred = global_disc(shared_fake_output)\n",
    "    global_real_pred = global_disc(shared_real_output)\n",
    "\n",
    "    global_fake_loss = adver_criterion(global_fake_pred, torch.zeros_like(global_fake_pred))\n",
    "    global_real_loss = adver_criterion(global_real_pred, torch.ones_like(global_real_pred))\n",
    "    \n",
    "    return (global_fake_loss + global_real_loss) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0390735d-d1ed-40e8-ba01-43e6e85ef844",
   "metadata": {},
   "source": [
    "## Patch Driscriminator Loss\n",
    "by  $$L_{p-adv} = BCELoss$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85c61ad-2bce-4609-b129-0149a3927ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patch_disc_loss(input_image, real_image, gen, shared, patch_disc, adver_criterion):\n",
    "    fake_image = gen(input_image)\n",
    "\n",
    "    shared_fake_output = shared(fake_image.detach())\n",
    "    shared_real_output = shared(real_image)\n",
    "    \n",
    "    patch_fake_pred = patch_disc(shared_fake_output)\n",
    "    patch_real_pred = patch_disc(shared_real_output)\n",
    "\n",
    "    patch_fake_loss = adver_criterion(patch_fake_pred, torch.zeros_like(patch_fake_pred))\n",
    "    patch_real_loss = adver_criterion(patch_real_pred, torch.ones_like(patch_real_pred))\n",
    "    \n",
    "    return (patch_fake_loss + patch_real_loss) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5319f7a0-d80e-4388-a150-7ec0dda3d67e",
   "metadata": {},
   "source": [
    "## Shared Layer Loss\n",
    "by $$L_{g-adv} + L_{p-adv}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f25304d-17da-4570-8698-9024f19d4d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shared_layer_loss(input_image, real_image, gen, shared, global_disc, patch_disc, adver_criterion):\n",
    "    fake_image = gen(input_image)\n",
    "\n",
    "    shared_fake_output = shared(fake_image.detach())\n",
    "    shared_real_output = shared(real_image)\n",
    "    \n",
    "    patch_fake_pred = patch_disc(shared_fake_output)\n",
    "    patch_real_pred = patch_disc(shared_real_output)\n",
    "\n",
    "    patch_fake_loss = adver_criterion(patch_fake_pred, torch.zeros_like(patch_fake_pred))\n",
    "    patch_real_loss = adver_criterion(patch_real_pred, torch.ones_like(patch_real_pred))\n",
    "\n",
    "    global_fake_pred = global_disc(shared_fake_output)\n",
    "    global_real_pred = global_disc(shared_real_output)\n",
    "\n",
    "    global_fake_loss = adver_criterion(global_fake_pred, torch.zeros_like(global_fake_pred))\n",
    "    global_real_loss = adver_criterion(global_real_pred, torch.ones_like(global_real_pred))\n",
    "\n",
    "    global_loss = (global_fake_loss + global_real_loss) / 2\n",
    "    patch_loss = (patch_fake_loss + patch_real_loss) / 2\n",
    "\n",
    "    return global_loss + patch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d7aecc-56aa-4248-ada4-e17972a54651",
   "metadata": {},
   "source": [
    "# Init Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908ca6ba-6ba3-4a44-bf8b-87a1229d08d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c3863e-da54-42d8-8b90-dab677858ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cafd40-4904-4580-b05c-dce833151eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = RasNet().to(device)\n",
    "gen_opt = torch.optim.Adam(params=gen.parameters(), lr=lr)\n",
    "\n",
    "shared_layer = SharedLayer().to(device)\n",
    "shared_opt = torch.optim.Adam(params=shared_layer.parameters(), lr=lr)\n",
    "\n",
    "global_disc = GlobalDiscriminator().to(device)\n",
    "global_opt = torch.optim.Adam(params=global_disc.parameters(), lr=lr)\n",
    "\n",
    "patch_disc = PatchDiscriminator().to(device)\n",
    "patch_opt = torch.optim.Adam(params=patch_disc.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ecc859-cb4a-4420-9f5c-46125d0b2e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = gen.apply(weights_init)\n",
    "shared_layer = shared_layer.apply(weights_init)\n",
    "global_disc = global_disc.apply(weights_init)\n",
    "patch_disc = patch_disc.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58365d05-7e5e-4dc8-8fa1-6bdaea8bf1c6",
   "metadata": {},
   "source": [
    "# Load pre-train model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a82f18-c46b-4ed3-966d-868c3d71cbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = RasNet().to(device)\n",
    "gen_opt = torch.optim.Adam(gen.parameters())\n",
    "global_disc = GlobalDiscriminator().to(device)\n",
    "global_opt = torch.optim.Adam(global_disc.parameters())\n",
    "shared_layer = SharedLayer().to(device)\n",
    "shared_opt = torch.optim.Adam(shared_layer.parameters())\n",
    "patch_disc = PatchDiscriminator().to(device)\n",
    "patch_opt = torch.optim.Adam(patch_disc.parameters())\n",
    "\n",
    "# Load the state dictionaries\n",
    "checkpoint = torch.load(\"pix2pix_210000.pth\")\n",
    "gen.load_state_dict(checkpoint['gen'])\n",
    "gen_opt.load_state_dict(checkpoint['gen_opt'])\n",
    "global_disc.load_state_dict(checkpoint['global_disc'])\n",
    "global_opt.load_state_dict(checkpoint['global_opt'])\n",
    "shared_layer.load_state_dict(checkpoint['shared_layer'])\n",
    "shared_opt.load_state_dict(checkpoint['shared_layer_opt'])\n",
    "patch_disc.load_state_dict(checkpoint['patch_disc'])\n",
    "patch_opt.load_state_dict(checkpoint['patch_disc_opt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feda4da7-7861-4907-8477-215dfe28086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "adver_criterion = nn.L1Loss()\n",
    "recon_criterion  = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e654017-9e89-4fe8-9e5b-5bf229f74274",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0e760e-4d5e-4d51-8d34-41c5e608565b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "cur_step = 0\n",
    "n_epochs = 20\n",
    "display_step = 100\n",
    "\n",
    "mean_global_loss = 0\n",
    "mean_patch_loss = 0\n",
    "mean_gen_loss = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for input_image, real_image in tqdm(dataloader):\n",
    "        \n",
    "        input_image = input_image.to(device)\n",
    "        real_image = real_image.to(device)\n",
    "\n",
    "        # Train Discriminator\n",
    "        global_opt.zero_grad()\n",
    "        global_loss = get_global_disc_loss(input_image, real_image, gen, shared_layer, global_disc, adver_criterion)\n",
    "        global_loss.backward(retain_graph=True)\n",
    "        global_opt.step()\n",
    "\n",
    "        patch_opt.zero_grad()\n",
    "        patch_loss = get_patch_disc_loss(input_image, real_image, gen, shared_layer, patch_disc, adver_criterion)\n",
    "        patch_loss.backward(retain_graph=True)\n",
    "        patch_opt.step()\n",
    "\n",
    "        shared_opt.zero_grad()\n",
    "        shared_loss = get_shared_layer_loss(input_image, real_image, gen, shared_layer, global_disc, patch_disc, adver_criterion)\n",
    "        shared_loss.backward(retain_graph=True)\n",
    "        shared_opt.step()\n",
    "\n",
    "        # Train Generator\n",
    "        gen_opt.zero_grad()\n",
    "        gen_loss = get_gen_loss(input_image, real_image, gen, shared_layer, global_disc, patch_disc, recon_criterion, adver_criterion)\n",
    "        gen_loss.backward()\n",
    "        gen_opt.step()\n",
    "\n",
    "        mean_global_loss = global_loss.item() / display_step\n",
    "        mean_patch_loss = patch_loss.item() / display_step\n",
    "        mean_gen_loss = gen_loss.item() / display_step\n",
    "        \n",
    "        if cur_step % display_step == 0 and cur_step > 0:\n",
    "            print(f'steps:{cur_step}, mean_gen_loss:{mean_gen_loss}, mean_global_loss:{mean_global_loss}, mean_patch_loss:{mean_patch_loss}')\n",
    "            fake_image = gen(input_image)\n",
    "            fake_image = denormalize(fake_image[0].detach().cpu())\n",
    "            input_image = denormalize(input_image[0].cpu())\n",
    "            real_image = denormalize(real_image[0].cpu())\n",
    "            fig, axs = plt.subplots(1, 3)\n",
    "            axs[0].imshow(fake_image.permute(1, 2, 0))\n",
    "            axs[0].axis(False)\n",
    "            axs[0].set_title('Generate')\n",
    "            axs[1].imshow(input_image.permute(1, 2, 0))\n",
    "            axs[1].axis(False)\n",
    "            axs[1].set_title('Input')\n",
    "            axs[2].imshow(real_image.permute(1, 2, 0))\n",
    "            axs[2].axis(False)\n",
    "            axs[2].set_title('Real')\n",
    "            plt.show()\n",
    "        cur_step += 1\n",
    "\n",
    "    torch.save({'gen': gen.state_dict(),\n",
    "        'gen_opt': gen_opt.state_dict(),\n",
    "        'global_disc': global_disc.state_dict(),\n",
    "        'global_opt': global_opt.state_dict(),\n",
    "        'shared_layer': shared_layer.state_dict(),\n",
    "        'shared_layer_opt': shared_opt.state_dict(),\n",
    "        'patch_disc': patch_disc.state_dict(),\n",
    "        'patch_disc_opt': patch_opt.state_dict()\n",
    "    }, f\"pix2pix_{cur_step}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
